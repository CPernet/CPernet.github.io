<html xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   
      <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      -->
      <title>Cyril Pernet GLM </title>
      <meta name="description" content="General linear model">
      <meta name="keywords" content="Cyril Pernet GLM Regressions">
      <meta name="generator" content="MATLAB 7.2">
      <meta name="date" content="2008-03-13">
      <meta name="m-file" content="cp_glm1"><style>

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows.  On Gecko-based browsers, the shrink-to-fit doesn't work. */ 
p,h1,h2,div.content div {
  /* for MATLAB's browser */
  width: 600px;
  /* for Mozilla, but the "width" tag overrides it anyway */
  max-width: 600px;
  /* for IE */
  width:expression(document.body.clientWidth > 620 ? "600px": "auto" );
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head>
   <body>
      <div class="content">
         <h2>Contents</h2>
         <div>
            <ul>
               <li><a href="../cp_stats.html">Home</a></li>
               <li><a href="#2">Linear correlations</a></li>
               <li><a href="#3">Examples of non linear correlations</a></li>
               <li><a href="#4">Simple regression</a></li>
               <li><a href="#5">Multiple regression</a></li>
               <li><a href="#6">Summary</a></li>
               <li><a href="cp_glm2.html">GLM2</a></li>
            </ul>
         </div><pre class="codeinput"><span class="comment">% In this first page I deal with simple and multiple regressions</span>
<span class="comment">% To start lets' have a look a linear correlations</span>

clc
clear <span class="string">all</span>
close <span class="string">all</span>
</pre><h2>Linear correlations<a name="2"></a></h2><pre class="codeinput"><span class="comment">% x a simple vector of numbers</span>
x = [-3 -2 -1 0 1 2 3];
plot(x,<span class="string">'b'</span>) <span class="comment">% blue line</span>

<span class="comment">% now we multiply by 2</span>
y = 2*x;
hold <span class="string">on</span>; plot(y,<span class="string">'r'</span>); <span class="comment">% red line</span>

<span class="comment">% add we add another vector</span>
z = y + [2 3 4 4 5 3 4];
plot(z,<span class="string">'g'</span>); <span class="comment">% green line</span>
hold <span class="string">off</span>

<span class="comment">% correlations are</span>
c_xy = corr([x', y']) <span class="comment">% default is Pearson correlation</span>
c_xz = corr([x', z']) <span class="comment">%</span>
c_yz = corr([y', z']) <span class="comment">%</span>

<span class="comment">% this illustrate the 2 properties of a linear system, namely</span>
<span class="comment">% scaling y = ax</span>
<span class="comment">% superposition y = x1 + x2</span>
</pre><pre class="codeoutput">
c_xy =

     1     1
     1     1


c_xz =

    1.0000    0.9863
    0.9863    1.0000


c_yz =

    1.0000    0.9863
    0.9863    1.0000

</pre><img vspace="5" hspace="5" src="cp_glm1_01.png"> <h2>Examples of non linear correlations<a name="3"></a></h2><pre class="codeinput">x = [-3 -2 -1 0 1 2 3];
plot(x,<span class="string">'b'</span>) <span class="comment">% blue line</span>

<span class="comment">% squarre the data</span>
y = x.^2;
hold <span class="string">on</span>
plot(y,<span class="string">'r'</span>) <span class="comment">% red line</span>

<span class="comment">% take the absolute values</span>
z = abs(x);
plot(z,<span class="string">'--g'</span>)<span class="comment">% green line</span>
hold <span class="string">off</span>

<span class="comment">% linear correlation coefficients are</span>
c_xy = corr([x', y'],<span class="string">'type'</span>,<span class="string">'Pearson'</span>)
c_xz = corr([x', z'],<span class="string">'type'</span>,<span class="string">'Pearson'</span>)
c_yz = corr([y', z'],<span class="string">'type'</span>,<span class="string">'Pearson'</span>)

<span class="comment">% but non-linear correlation tells us that there is a relation</span>
c_xy2 = corr([x', y'],<span class="string">'type'</span>,<span class="string">'Kendall'</span>)
c_xz2 = corr([x', z'],<span class="string">'type'</span>,<span class="string">'Kendall'</span>)
c_yz2 = corr([y', z'],<span class="string">'type'</span>,<span class="string">'Kendall'</span>)
</pre><pre class="codeoutput">
c_xy =

    1.0000   -0.0000
   -0.0000    1.0000


c_xz =

    1.0000   -0.0000
   -0.0000    1.0000


c_yz =

    1.0000    0.9608
    0.9608    1.0000


c_xy2 =

     1     0
     0     1


c_xz2 =

     1     0
     0     1


c_yz2 =

     1     1
     1     1

</pre><img vspace="5" hspace="5" src="cp_glm1_02.png"> <h2>Simple regression<a name="4"></a></h2><pre class="codeinput"><span class="comment">% In simple regression, we want to explain the data (y) by a single predictor (x) such as y = beta*x + b</span>
<span class="comment">% It is solved by the least squares method, i.e. one looks for a coefficient (beta) that minimizes the error,</span>
<span class="comment">% i.e. the difference between the model (beta*x+b) and the data (y). To this purpose we use the polyfit function</span>
<span class="comment">% [p,S,mu] = polyfit(x,y,n). For a linear regression (a line) the polynome order is 1.</span>
<span class="comment">% p = polyfit(x,y,1) ? y = p(1)x + p(2)</span>

clear <span class="string">all</span>
<span class="comment">% fisrt we create a model we know + some random variations</span>
x = [-3 -2 -1 0 1 2 3];
y  = 3 * x + 5 + randn(1,7);

<span class="comment">% clearly x and y are linearly correlated (x is simply scaled + cst)</span>
[r,p]=corr([y', x']);
plot(x,<span class="string">'b'</span>); hold <span class="string">on</span>
plot(y,<span class="string">'r'</span>); hold <span class="string">off</span>
title([<span class="string">'correlation [xy]='</span>,num2str(r(2)),<span class="string">'p='</span>,num2str(p(2))]);

<span class="comment">% now we look for beta (i.e. about 3) and the cst term (about 5)</span>
p = polyfit(x,y,1);

<span class="comment">% we can then run the model using polyval</span>
model = polyval(p,x);
figure; plot (x,y,<span class="string">'o'</span>,x,model);
title([<span class="string">'y = '</span>,num2str(p(1)),<span class="string">'x + '</span>,num2str(p(2))]);

<span class="comment">% we can now look at how good the model explains</span>
<span class="comment">% the data</span>

<span class="comment">% fisrt test the model, i.e. evaluate if it is statistically</span>
<span class="comment">% significant = is the model explain more the data than the error</span>
<span class="comment">% all tests are evaluated as a ratio prediction / error</span>

df = rank(x); <span class="comment">% we will come back on the rank function latter</span>
ss_effect = norm(model - mean(model)).^2; <span class="comment">% the norm of the vector is it's length</span>
dfe = length(y)-rank(x)-1; <span class="comment">% nb of observation - nb of explained variables</span>
residuals = y - model;
ss_error = norm(residuals).^2;
f  = (ss_effect/df) / (ss_error/dfe);
pval = 1 - fcdf(f, df, dfe);

<span class="comment">% plot the data and the model</span>
subplot(1,3,1); plot(y); hold <span class="string">on</span>; plot(model,<span class="string">'r'</span>)
title([<span class="string">'F('</span>,num2str(df),<span class="string">','</span>,num2str(dfe),<span class="string">')='</span>,num2str(f), <span class="string">' p='</span>,num2str(pval)])

<span class="comment">% plot the residuals - if y is normally distributed</span>
<span class="comment">% the residuals are also normally distributed</span>
subplot(1,3,2); ksdensity(residuals);
[H,P] = kstest(y); <span class="comment">% do the Kolmogorov test</span>
title([<span class="string">'Normality p='</span>,num2str(P)]);
subplot(1,3,3); normplot(residuals);
</pre><img vspace="5" hspace="5" src="cp_glm1_03.png"> <img vspace="5" hspace="5" src="cp_glm1_04.png"> <h2>Multiple regression<a name="5"></a></h2><pre class="codeinput"><span class="comment">% Again we want to explain the data (y) but this time by several predictors (x1, x2, &#8230;) such as y = beta1*x1 + beta2*x2 + b</span>
<span class="comment">% Again this is solved by the least squares method, i.e. one looks for coefficients (betas) that minimizes the error,</span>
<span class="comment">% i.e. the difference between (beta1*x1 + beta2*x2 + b) and y - Note that instead of a line, we will have a plan if we have</span>
<span class="comment">% 2 regressors, a space with 3 regressors and hyperspaces when &gt; 3 regressors</span>

<span class="comment">% Matlab has a build in function regress that gives all we need</span>
clear
load <span class="string">hald</span>;
Y=hald(:,5) ;
X=hald(:,1:4);
X = [ones(length(X),1) X]; <span class="comment">% we simply add a column of ones for the cst term</span>
[b,bint,r,rint,stats] = regress(Y,X,0.5); <span class="comment">% gives the solutions for all betas as well as the stats</span>

<span class="comment">% Again we can have a look at the data, the model and the residuals</span>
subplot(1,2,1); plot(Y);
model = X*b;
hold <span class="string">on</span>; plot(model,<span class="string">'r'</span>);
title([<span class="string">'F('</span>,num2str(rank(X)-1),<span class="string">','</span>,num2str(length(Y)-rank(X)),<span class="string">')='</span>,num2str(stats(2)),<span class="string">' p='</span>,num2str(stats(3))])
residuals = Y - model;
subplot(1,2,2); normplot(residuals);

<span class="comment">% We could also solve this problem easily by hand:</span>
<span class="comment">% 1. for an equation like y = ax the solution is a = y*1/x</span>
<span class="comment">% 2. for a set a numbers in y and x the predictors betas = y*inv(X)</span>
<span class="comment">% note that 1/X is not = to inv(X); inv(X) is a matrix such as X*inv(X)=I (I being the identity matrix)</span>
<span class="comment">% 3. the problem here is that X is not a square matrix, and has therefore no inverse, but we can get</span>
<span class="comment">% around it by multiplying by X' such as X'*Y = X'*X*beta and thus beta = inv(X'*X)*X'*Y</span>
<span class="comment">% note that matrix multiplication is the sum of the product of lines per columns i.e. X'*X is a sum of squares</span>

beta = inv(X'*X)*X'*Y; <span class="comment">% get the coeficients</span>
Yhat = X*beta; <span class="comment">% estimated Y (model)</span>
r    = Y-Yhat; <span class="comment">% residuals</span>

ss_total  = norm(Y - mean(Y)).^2; <span class="comment">% SS total</span>
ss_effect = norm(Yhat - mean(Yhat)).^2; <span class="comment">% SS effect</span>
ss_error  = norm(r).^2; <span class="comment">% SS error</span>

rsquare = 1 - ss_error/ss_total <span class="comment">% fitting statistic R2 = part of explained variance</span>
f       = (ss_effect/(rank(X)-1)) / (ss_error/(length(Y)-rank(X))) <span class="comment">% F effect/error</span>
pval    = 1 - fcdf(f, (rank(X)-1), (length(Y)-rank(X))) <span class="comment">% p val</span>
</pre><pre class="codeoutput">
rsquare =

    0.9824


f =

  111.4792


pval =

  4.7562e-007

</pre><img vspace="5" hspace="5" src="cp_glm1_05.png"> <h2>Summary<a name="6"></a></h2><pre class="codeinput"><span class="comment">% matlab functions</span>
<span class="comment">% correlations linear or not: [r,p]=corr(X,'type',' ');</span>
<span class="comment">% fitting a straight line and get the model: p = polyfit(x,y,1); model = polyval(p,x);</span>
<span class="comment">% fitting a plan/hyperplan: [b,bint,r,rint,stats] = regress(Y,X,alpha);</span>
<span class="comment">% normality plot: normplot</span>
<span class="comment">%</span>
<span class="comment">% linear model</span>
<span class="comment">% Y = betas*X + b is the general from of any linear model</span>
<span class="comment">% the ordinary least squarre solution is given by betas = inv(X'*X)*X'*Y</span>
</pre><p class="footer"><br>
            Published with MATLAB&reg; 7.2<br></p>
      </div>
      <!--
##### SOURCE BEGIN #####
% In this first page I deal with simple and multiple regressions
% To start lets' have a look a linear correlations

clc
clear all
close all


%% Linear correlations


% x a simple vector of numbers
x = [-3 -2 -1 0 1 2 3];
plot(x,'b') % blue line

% now we multiply by 2
y = 2*x;
hold on; plot(y,'r'); % red line

% add we add another vector
z = y + [2 3 4 4 5 3 4];
plot(z,'g'); % green line
hold off

% correlations are
c_xy = corr([x', y']) % default is Pearson correlation
c_xz = corr([x', z']) %  
c_yz = corr([y', z']) %

% this illustrate the 2 properties of a linear system, namely
% scaling y = ax
% superposition y = x1 + x2


%% Examples of non linear correlations 


x = [-3 -2 -1 0 1 2 3];
plot(x,'b') % blue line

% squarre the data
y = x.^2; 
hold on
plot(y,'r') % red line

% take the absolute values
z = abs(x);
plot(z,'REPLACE_WITH_DASH_DASHg')% green line
hold off

% linear correlation coefficients are
c_xy = corr([x', y'],'type','Pearson')
c_xz = corr([x', z'],'type','Pearson')
c_yz = corr([y', z'],'type','Pearson')

% but non-linear correlation tells us that there is a relation
c_xy2 = corr([x', y'],'type','Kendall')
c_xz2 = corr([x', z'],'type','Kendall')
c_yz2 = corr([y', z'],'type','Kendall')


%% Simple regression


% In simple regression, we want to explain the data (y) by a single predictor (x) such as y = beta*x + b
% It is solved by the least squares method, i.e. one looks for a coefficient (beta) that minimizes the error, 
% i.e. the difference between the model (beta*x+b) and the data (y). To this purpose we use the polyfit function
% [p,S,mu] = polyfit(x,y,n). For a linear regression (a line) the polynome order is 1.
% p = polyfit(x,y,1) ? y = p(1)x + p(2)

clear all
% fisrt we create a model we know + some random variations
x = [-3 -2 -1 0 1 2 3];
y  = 3 * x + 5 + randn(1,7);

% clearly x and y are linearly correlated (x is simply scaled + cst)
[r,p]=corr([y', x']);
plot(x,'b'); hold on
plot(y,'r'); hold off
title(['correlation [xy]=',num2str(r(2)),'p=',num2str(p(2))]);

% now we look for beta (i.e. about 3) and the cst term (about 5)
p = polyfit(x,y,1); 

% we can then run the model using polyval
model = polyval(p,x); 
figure; plot (x,y,'o',x,model);
title(['y = ',num2str(p(1)),'x + ',num2str(p(2))]);

% we can now look at how good the model explains
% the data

% fisrt test the model, i.e. evaluate if it is statistically
% significant = is the model explain more the data than the error
% all tests are evaluated as a ratio prediction / error

df = rank(x); % we will come back on the rank function latter
ss_effect = norm(model - mean(model)).^2; % the norm of the vector is it's length
dfe = length(y)-rank(x)-1; % nb of observation - nb of explained variables
residuals = y - model;
ss_error = norm(residuals).^2;
f  = (ss_effect/df) / (ss_error/dfe);
pval = 1 - fcdf(f, df, dfe);

% plot the data and the model
subplot(1,3,1); plot(y); hold on; plot(model,'r')
title(['F(',num2str(df),',',num2str(dfe),')=',num2str(f), ' p=',num2str(pval)])

% plot the residuals - if y is normally distributed
% the residuals are also normally distributed
subplot(1,3,2); ksdensity(residuals); 
[H,P] = kstest(y); % do the Kolmogorov test 
title(['Normality p=',num2str(P)]);
subplot(1,3,3); normplot(residuals);


%% Multiple regression


% Again we want to explain the data (y) but this time by several predictors (x1, x2, …) such as y = beta1*x1 + beta2*x2 + b
% Again this is solved by the least squares method, i.e. one looks for coefficients (betas) that minimizes the error,
% i.e. the difference between (beta1*x1 + beta2*x2 + b) and y - Note that instead of a line, we will have a plan if we have
% 2 regressors, a space with 3 regressors and hyperspaces when > 3 regressors

% Matlab has a build in function regress that gives all we need
clear
load hald; 
Y=hald(:,5) ; 
X=hald(:,1:4); 
X = [ones(length(X),1) X]; % we simply add a column of ones for the cst term
[b,bint,r,rint,stats] = regress(Y,X,0.5); % gives the solutions for all betas as well as the stats

% Again we can have a look at the data, the model and the residuals
subplot(1,2,1); plot(Y);
model = X*b; 
hold on; plot(model,'r');
title(['F(',num2str(rank(X)-1),',',num2str(length(Y)-rank(X)),')=',num2str(stats(2)),' p=',num2str(stats(3))])
residuals = Y - model;
subplot(1,2,2); normplot(residuals);

% We could also solve this problem easily by hand:
% 1. for an equation like y = ax the solution is a = y*1/x
% 2. for a set a numbers in y and x the predictors betas = y*inv(X) 
% note that 1/X is not = to inv(X); inv(X) is a matrix such as X*inv(X)=I (I being the identity matrix)
% 3. the problem here is that X is not a square matrix, and has therefore no inverse, but we can get
% around it by multiplying by X' such as X'*Y = X'*X*beta and thus beta = inv(X'*X)*X'*Y
% note that matrix multiplication is the sum of the product of lines per columns i.e. X'*X is a sum of squares 

beta = inv(X'*X)*X'*Y; % get the coeficients
Yhat = X*beta; % estimated Y (model) 
r    = Y-Yhat; % residuals

ss_total  = norm(Y - mean(Y)).^2; % SS total
ss_effect = norm(Yhat - mean(Yhat)).^2; % SS effect
ss_error  = norm(r).^2; % SS error 

rsquare = 1 - ss_error/ss_total % fitting statistic R2 = part of explained variance
f       = (ss_effect/(rank(X)-1)) / (ss_error/(length(Y)-rank(X))) % F effect/error
pval    = 1 - fcdf(f, (rank(X)-1), (length(Y)-rank(X))) % p val


%% Summary

% matlab functions
% correlations linear or not: [r,p]=corr(X,'type',' ');
% fitting a straight line and get the model: p = polyfit(x,y,1); model = polyval(p,x); 
% fitting a plan/hyperplan: [b,bint,r,rint,stats] = regress(Y,X,alpha);
% normality plot: normplot
% 
% linear model
% Y = betas*X + b is the general from of any linear model 
% the ordinary least squarre solution is given by betas = inv(X'*X)*X'*Y




##### SOURCE END #####
-->
   </body>
</html>